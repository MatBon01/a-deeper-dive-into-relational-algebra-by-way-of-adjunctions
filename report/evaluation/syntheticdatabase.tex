\section{Synthetic database creation}
This section evaluates the design decisions and outcomes in creating a synthetic
database generator as outlined in \fref{sec:benchmark:syntheticdatabase}. This
project introduced a framework to generate database to fit a custom schema. The
benefits of synthetic data are vast and include the ability to define schemas
that are specialised in benchmarking and readability while controlling frequency
of values and other important properties.

Overall the result was quite different from other technologies available,
offering a low-level bare-bones solution. Many existing solutions equally support
qualitative data to quantitative with selling points based on the
\lstinline{Faker} python module. Although \lstinline{Faker} did not come to my
attention while designing the framework, it would not have been much use for the
\database{JOINBENCH} database in its final form as well. Additionally, much
infrastructure would still need to be made around the \lstinline{Faker} module
for easy exporting and specification, therefore it would not have saved much
time and allowed me to develop a more customised solution for the problem.
Advantages of the \lstinline{Faker} framework over my solution is the vast
amount of data available for attributes that were considered to be qualitative,
although there would still be time necessary in order to do due diligence on its
copyright and licensing creating further ethical considerations. The \lstinline{Faker} module has made interesting
design decisions, favouring a \lstinline{unique} attribute to composition for
uniqueness which itself comes with benefits and drawbacks. The \lstinline{Faker}
module does present interesting opportunities to enhance my solution however:
Using an adapter design pattern a faker object could easily be converted into
using the cell API and enrich my solution with a larger variety of data,
although I would accept the argument that it might be a waste of the set of rich
features implemented by the module. Using the \lstinline{Faker} module implicity
with other data synthesising modules that depend on
it, such as \lstinline{pydbgen}, can give a more complete solution even in
comparison to my framework. In essence, it uses the variety of data sources
provided by framework compiled into a variety of different outputs formats.

Moreover, other solutions with very different aims exist. Academics in the US
have developed DataSynthesizer \cite{DataSynthesizer} which is a privacy
oriented synthetic data generator for collaborating with sensitive data. Given
an input of a sensitive data the technology comes in three parts, a
\lstinline{DataDescriber} which analyses the data and describes its form, a
\lstinline{DataGenerator} which takes this information and creates a
predetermined number of rows, and finally a \lstinline{ModelInspector} for
determining the similarity of the produced data to the private data. It is clear
in order to use the technology as intended I would already require a dataset to
reproduce. Hypothetically, singling out the \lstinline{DataGenerator} and
providing a hand-written description may also not be a very specialised
solution. Despite the fact that it is not the intended purpose of the system,
the generation is clearly created to be able to represent real world data, which
the \relation{JOINBENCH} relation is far from. Therefore, other more specialised
solutions may be more useful.
