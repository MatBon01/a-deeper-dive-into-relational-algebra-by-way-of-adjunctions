\chapter{Introduction}
\begin{comment}
The introduction should summarise the subject area, the specific problem you are addressing, including key ideas for their solution, together with a summary of the project's main contributions. When detailing the contributions it is helpful to provide forward references to the section(s) of the report that provide the relevant technical details. The introduction should be aimed at an informed, but otherwise non-expert, reader. A good tip is to assume that all your assessors will read the abstract and introduction, whereas the more detailed technical sections may only be read by your first and second markers - it's therefore really important to get it right.
\end{comment}

This project follows the consequences of new techniques to optimise
relational queries of joins followed by selections in Haskell. Say there are the
following records in Haskell.

\input{code/introRecordExample.tex}

\noindent
How might one join two bags of these records together by id? One way would be to
write the following.

\[\lbag\:(s, g)\:|\:s \leftarrow students,\:g \leftarrow grades,\:uid\;s == sid\;g
\:\rbag\]

What are the disadvantages of this approach? Clearly, every possible pair of
elements are considered regardless of whether they have matching ids or not.
This has an asymptotic complexity of $O(nm)$ where $n$ and $m$ are the
cardinality of the $students$ and $grades$ bags respectively.

Databases are vital to modern-day society for their ability to structure, sort
and query vast amounts of data from any domain. Of course, many theoretical
models of data have surfaced since its conception, with different approaches
describing how exactly to hold the data. Examples of such data models include
the semi-structured model~\cite{DatabaseSystems} and, more relevant to this
project, the relational model~\cite{RelationalModel} as introduced in
\fref{sec:relationalmodel}. However, all implementations face trade-offs and
development challenges.

This project explores one such solution to the optimisation of queries
consisting of products followed by selections in the relational model. Such
behaviour can be modelled by equijoins, the type of join seen in the example
above, where you wish to keep only two records that share a common attribute.
This project will implement a functioning database system in Haskell according
to the solution outlined in the paper
\relalg{}~\cite{RelationalAlgebraByWayOfAdjunctions}. It then describes original
tools and workflows created to help the benchmarking and testing of the system
in order to evaluate its performance.

Going back to the example, say it was possible to instead index both tables by their
$uid$ and $sid$ respectively. One would not need to consider all possible pairs
of elements to find the matching attributes any more as which tuples share the
same indexed element would already be known. This will allow the developer to greatly reduce the
order in which they are taking products and, in effect, `making more work for
themselves'. How might this be achieved? The authors of the paper \relalg{},
seemingly enchanted by the elegance of a comprehension notation, have outlined a
category theoretical framework to interpret relational algebra. They found that
many bulk types, such as the bags above, could be made by the adjunction of two
functors and therefore granted them the properties of monads. Moreover, monad
comprehensions have been known for a substantial amount of
time~\cite{MonadComprehensions} and are a generalisation of the expressive list
comprehension. Therefore, in conjunction with the SQL-like syntax extensions
available to list comprehensions~\cite{ComprehensiveComprehensions} it is hoped
that the framework defined in the paper for an efficient equijoin can be
expressed in the same concise and powerful manner as the example above.

Continuing from the example above, how might one more efficiently join the
two relations? They could start by indexing both relations by the attributes the equijoin
is targeting and merging the results.

\begin{center}
\input{code/indexSchoolAndGrade}
\end{center}

\noindent
After this they simply could perform a Cartesian product on all values matching keys
and reduce the answer back to a bag.

\begin{center}
\input{code/reduceMergedSchoolAndGrade}
\end{center}

\noindent
Note the types of the functions used. The types will be modified from their general
form in order to better inform their use in this restricted context.

\input{code/introFunctionTypes.tex}

\noindent
The example demonstrates that the need for a global Cartesian product is removed by localising
the operation on to values that definitely require it. This is accomplished by
indexing both relations entirely, to an intermediate $Map$ form. The merging of
these maps is responsible for matching the keys, the values requested in the
equijoin, and then a Cartesian product is conducted on each of these pairings
individually through the $fmap$. In order to be a valid relational operation,
however, the result must be reduced back to the database implementation and
therefore the $Map$ is transformed back to a $Bag$.

Theoretically, this indexing should help reduce the asymptotic complexity of
the algorithm. Practically, however, what are the results of such a method? Are
there any limitations? Does it see the gains theorised in real life? This
project is about exploring exactly that idea.

After describing a working implementation of the database system mentioned here,
this report is going to describe the bespoke and original tools created in order
to benchmark the database. We introduce the \relation{JOINBENCH} relation, a
specialised version of past academic benchmarking database schemas made with
testing equijoins in mind. After this, the report will describe the design of
an extensible tool created in order to generate synthetic databases to run the
benchmarks on. This tool can be very easily customised and is made with enforcing
mathematical properties of data in mind so that interesting patterns can emerge
from the \relation{JOINBENCH} relation. Next, an overview of the benchmarking
workflow and experiment designs is given to be open and encourage readers to try
and reproduce results. Finally, a discussion of the results is given and the
efficacy of replacing such a solution is discussed.

Synthetic data generation is key to creating a readable and reproducible
benchmark. Generating synthetic data allows us to specify the properties of
attributes and thereby conduct specific experiments determining the pitfalls and
strengths of functions depending on the shape of the data. Of course, there is
the argument to be made that the results are not `life-like' as they do not take
the form of real-world data. However, I believe that this way of doing it is not
only more archetypal and therefore gives us an insight into the underlying trends
and lesson to be learned about the query optimisation but it counter-intuitively
is more readable. In order to synthesise such data, a low-level synthetic
database-generating library was created within the scope of the project. This is
a framework that allows users to specify trends between attributes and control
the generation of cells within their domain. Furthermore, a \relation{JOINBENCH}
relation was inspired by similar work in the past to include attributes that I
believe most suitably tested and described join-based benchmarking queries.

To summarise, the main contributions of this project are:
\begin{itemize}
    \item a working implementation of the database system outlined in the paper
        \relalg{},
    \item the definition of the \relation{JOINBENCH} relation specialised from
        old benchmarking databases to provide all the tools needed to test
        equijoin,
    \item a completely new tool to generate any number of general synthetic data
        sets,
    \item an analysis of the results of the benchmarks on the database
        management system described above to evaluate its performance and
        success.
\end{itemize}
