\section{Benchmark workflow}\label{sec:benchmark:workflow}
In order to efficiently benchmark and analyse the performance of the database
management system a workflow system was created to automate many of the tasks
involved. This section will briefly describe the workflow and its creation.

There are multiple steps involved in the benchmarking of the database management
systems. The first step is to create synthetic version of the
\relation{JOINBENCH} relation up to the specifications of the benchmark.
Alternatively, one relation at least as large as the highest cardinality
interested in benchmarking can be made and then selections the
\relationAttribute{unique} attributes can be used to alter the cardinality;
however, this involved extra computation on the side of the system to benchmark
and therefore I decided to create a synthetic relation for each tuple count.
After the relations are created, they must be parsed into the Haskell program
and loaded into the database. All the queries can then be run and benchmarked.
Finally, the data is processed and the relevant statistics and figures are
extracted and created.

The workflow is largely managed through use of a Makefile. The advantage of this
approach is making sure that a table is always generated for the benchmark to
run and that unneccesary computation is kept at a minimum. Below is an example
of the most important rules in the Makefile; a rule to run the benchmark and a
rule to generate the tables.

\begin{lstlisting}[language=make]
joinbench%: $(tables_path)/joinbenchtable%.csv
	cabal run joinbench-benchmark -- $(tables_path)/joinbenchtable$*.csv \ 
    --template json --output data/joinbench$*.json

$(tables_path)/joinbenchtable%.csv:
	python3 $(database_gen_path)/generate_JOINBENCH_database.py \
    $* ${tables_path}/joinbenchtable$*.csv
\end{lstlisting}

The target of the first rule is a phony target set to run a benchmark with a
given number of tuples. If the command \lstinline[language=bash]{make
joinbench150} is run, a benchmark will be run on a database with 150 tuples. In
order to run the benchmark, the prerequisite of the target must be satisfied and
so if there is not a table with 150 tuples already in the tables directory
another is made.

As you can see from the Makefile, the tables generated are in csv form. This is
not a form which can be directly loaded into the database management system and
so a parser is required to convert the csv file into bags that are expected from
my implementation of the Haskell program. The parser is written in Haskell and
separates lines as elements of the bag and ``cells'' (values separated by
commas) to be either integers or $Word16$s. As I will not join on the
\relationAttribute{unique} attribute, it is specified as an integer and all
others $Word16$s for limitations discussed in \fref{ch:implementation}. For
readability of benchmarks during development the final type the
\relation{JOINBENCH} relation takes is a bag of records as specified in
\fref{sec:benchmark:database}.

After the database has been parsed the necessary benchmarks are run by the
\verb|Criterion| library and a file is produced in a JSON format (as can be seen
in the Makefile). This file is stored in a designated \verb|data| directory and
can be automatically loaded in by my statistical analysis python module. This
module describes some useful abstractions to reference parts of the data used
most often and utility functions for automatically loading, grouping and finding
data. Furthermore, there are some graphing classes that have been created to
automatically generate graphs in this paper and other that help understand the
data before writing. In order to run this code I used an non-automated approach
by creating a Jupyter notebook. Originally the notebook was meant to be used for
preliminary data analysis and I planned to easily write a script to automate the
generation of figures in my CICD pipeline made easier by virtue of the fact that 
all of the difficult code was written in a dedicated python module so would not
need to be duplicated. However, I found the ability to check the quality of the
graphs in an interactive environment before producing an output to be very
useful and so kept the notebook as a quality assurance mechanism.
